{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Reddit Data Ingestion\n",
    "\n",
    "For this exercise, you are going to ingest Reddit RSS feeds into a PostgreSQL database structure of your design.\n",
    "\n",
    "This exercise will rely on some lessons from the reading in this module, but heavily on the prior modules and some prior coursework in Python, this course and DB/SQL boot camps.\n",
    "\n",
    "*** Remember to break this down into manageable tasks and not attempt to build the entire database before knowing one of the entities and its attributes. ***\n",
    "\n",
    "\n",
    "### From the site:\n",
    "\n",
    "reddit: the front page of the internet  \n",
    "https://www.reddit.com/  \n",
    "Reddit gives you the best of the internet in one place. Get a constantly updating feed of breaking news, fun stories, pics, memes, and videos just for you.\n",
    "\n",
    "\n",
    "### From Wikipedia:\n",
    "Reddit is an American social news aggregation, web content rating, and discussion website. \n",
    "Registered members submit content to the site such as links, text posts, and images, \n",
    "which are then voted up or down by other members. \n",
    "Posts are organized by subject into user-created boards called \"subreddits\", \n",
    "which cover a variety of topics including news, science, movies, video games, music, books, fitness, food, and image-sharing. \n",
    "Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough votes, ultimately on the site's front page. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Posting:\n",
    "\n",
    "The below link is an example discussion that was started based on someone asking for opinions on MySQL vs NoSQL.  \n",
    "\n",
    "**Spoiler: ** The conclusion was PostgreSQL, leveled with a healthy dose of sarcasm that was lost on the original poster.\n",
    "\n",
    "https://www.reddit.com/r/nosql/comments/8ckkzg/should_i_use_nosql/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From: https://www.redditinc.com/\n",
    "![REDDIT_About.png MISSING](../images/REDDIT_About.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Really Simple Syndication (RSS)\n",
    "\n",
    "AKA: Rich Site Summary; [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) Site Summary\n",
    "\n",
    "**From Wikipedia**  \n",
    "RSS is a type of web feed which allows users to access updates to online content in a **standardized, computer-readable format**. \n",
    "These feeds can, for example, allow a user to keep track of many different websites in a single news aggregator. \n",
    "The news aggregator will automatically check the RSS feed for new content, allowing the content to be automatically passed from website to website or from website to user. \n",
    "This passing of content is called web syndication. \n",
    "Websites usually use RSS feeds to publish frequently updated information, such as blog entries, news headlines, audio, video. An RSS document (called \"feed\", \"web feed\", or \"channel\") includes full or summarized text, and metadata, like publishing date and author's name.\n",
    "\n",
    "#### Reddit supports RSS access to their community of posts.\n",
    "\n",
    "Click the link below to see the new Reddit posts feed in raw form (XML)\n",
    " * https://www.reddit.com/new/.rss?sort=new\n",
    " \n",
    "This is an example of a sub-reddit RSS feed:\n",
    " * https://www.reddit.com/r/datascience/.rss?sort=new\n",
    " \n",
    "**In both cases, we see the pattern of after the URL's last slash, \"/\", we add**  \n",
    "  \n",
    "`.rss?sort=new`\n",
    "\n",
    "\n",
    "**The wall of character data you see should scream: _Parse Me with Python!_**\n",
    "\n",
    "#### RSS Feed Content\n",
    "\n",
    "The RSS Feed is structured as a set of items, which can roughly be expected to follow the below structure. \n",
    "Note: the XML has been parsed into a DOM, then renderd as a JSON here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```JSON\n",
    "{\n",
    "\t'guidislink': True, \n",
    "\t'author_detail': {\n",
    "\t\t'href': 'https://www.reddit.com/user/cryoskyd', \n",
    "\t\t'name': '/u/cryoskyd'\n",
    "\t\t}, \n",
    "\t'links': [\n",
    "\t\t{\n",
    "\t\t\t'rel': 'alternate', \n",
    "\t\t\t'type': 'text/html', \n",
    "\t\t\t'href': 'https://www.reddit.com/r/SkydTech/comments/8r47kj/dealmaster_get_a_15inch_dell_laptop_with_an/'\n",
    "\t\t}], \n",
    "\t'href': 'https://www.reddit.com/user/cryoskyd', \n",
    "\t'updated_parsed': time.struct_time(tm_year=2018, tm_mon=6, tm_mday=14, tm_hour=18, tm_min=38, tm_sec=25, tm_wday=3, tm_yday=165, tm_isdst=0), \n",
    "\t'authors': [\n",
    "\t\t{'href': 'https://www.reddit.com/user/cryoskyd', \n",
    "\t\t'name': '/u/cryoskyd'\n",
    "\t\t}\n",
    "\t\t], \n",
    "\t'tags': [\n",
    "\t\t{'label': 'r/SkydTech', \n",
    "\t\t'scheme': None, \n",
    "\t\t'term': 'SkydTech'\n",
    "\t\t}], \n",
    "\t'title_detail': {\n",
    "\t\t'type': 'text/plain', \n",
    "\t\t'base': 'https://www.reddit.com/new/.rss?sort=new', \n",
    "\t\t'value': 'Dealmaster: Get a 15-inch Dell laptop with an 8th-gen Core i7 for $580', \t\t\t\t'language': None\n",
    "\t\t}, \n",
    "\t'summary': '<table> <tr><td> <a href=\"https://www.reddit.com/r/SkydTech/comments/8r47kj/dealmaster_get_a_15inch_dell_laptop_with_an/\"> <img src=\"https://b.thumbs.redditmedia.com/ioyXj08RjCyhRbNWiPQfcjrQMlHTG4Ec-LrYJ6MB0kI.jpg\" alt=\"Dealmaster: Get a 15-inch Dell laptop with an 8th-gen Core i7 for $580\" title=\"Dealmaster: Get a 15-inch Dell laptop with an 8th-gen Core i7 for $580\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryoskyd\"> /u/cryoskyd </a> &#32; to &#32; <a href=\"https://www.reddit.com/r/SkydTech/\"> r/SkydTech </a> <br/> <span><a href=\"https://arstechnica.com/staff/2018/06/dealmaster-get-a-15-inch-dell-laptop-with-an-8th-gen-core-i7-for-580/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/SkydTech/comments/8r47kj/dealmaster_get_a_15inch_dell_laptop_with_an/\">[comments]</a></span> </td></tr></table>', \n",
    "\t'id': 'https://www.reddit.com/new/t3_8r47kj', \n",
    "\t'updated': '2018-06-14T18:38:25+00:00', \n",
    "\t'content': [\n",
    "\t\t{\n",
    "\t\t\t'type': 'text/html', \n",
    "\t\t\t'base': 'https://www.reddit.com/new/.rss?sort=new', \n",
    "\t\t\t'value': '<table> <tr><td> <a href=\"https://www.reddit.com/r/SkydTech/comments/8r47kj/dealmaster_get_a_15inch_dell_laptop_with_an/\"> <img src=\"https://b.thumbs.redditmedia.com/ioyXj08RjCyhRbNWiPQfcjrQMlHTG4Ec-LrYJ6MB0kI.jpg\" alt=\"Dealmaster: Get a 15-inch Dell laptop with an 8th-gen Core i7 for $580\" title=\"Dealmaster: Get a 15-inch Dell laptop with an 8th-gen Core i7 for $580\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/cryoskyd\"> /u/cryoskyd </a> &#32; to &#32; <a href=\"https://www.reddit.com/r/SkydTech/\"> r/SkydTech </a> <br/> <span><a href=\"https://arstechnica.com/staff/2018/06/dealmaster-get-a-15-inch-dell-laptop-with-an-8th-gen-core-i7-for-580/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/SkydTech/comments/8r47kj/dealmaster_get_a_15inch_dell_laptop_with_an/\">[comments]</a></span> </td></tr></table>', \n",
    "\t\t\t'language': None\n",
    "\t\t}], \n",
    "\t'link': 'https://www.reddit.com/r/SkydTech/comments/8r47kj/dealmaster_get_a_15inch_dell_laptop_with_an/', \n",
    "\t'author': '/u/cryoskyd', \n",
    "\t'title': 'Dealmaster: Get a 15-inch Dell laptop with an 8th-gen Core i7 for $580'\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process this data we are going to use some key Python Libraries:\n",
    " * FeedParser\n",
    "   * https://pypi.org/project/feedparser/\n",
    "   * http://www.pythonforbeginners.com/feedparser/using-feedparser-in-python\n",
    " * BeautifulSoup\n",
    "   * https://www.crummy.com/software/BeautifulSoup/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code:\n",
    "\n",
    "The example code below grabs the new reddit posts from the RSS feed, then prints the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': [{'href': 'https://www.reddit.com/user/iptvglobal', 'name': '/u/iptvglobal'}], 'title': 'BUYIP-TV.COM Offer iptv trial', 'tags': [{'label': 'r/myIPTV', 'term': 'myIPTV', 'scheme': None}], 'guidislink': True, 'summary': '<!-- SC_OFF --><div class=\"md\"><h2>1-Day Risk <a href=\"https://www.buyip-tv.com/free-trial/\">Free trial</a>. No obligation, no credit card required. Simply Request <a href=\"https://www.buyip-tv.com/free-trial/\">free</a> <a href=\"https://www.buyip-tv.com/free-trial/\">IPTV Trial</a> and you’ll be up and running within few min.</h2> <p>Are you still confused about getting an IPTV subscription? Do you want an <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>iptv test</strong></a> of the service before paying a huge amount?<br/> <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>IPTV 24-h test</strong></a> Subscription is your solution to get a full test of the <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>iptv</strong></a> serivce and to be sure you are subscribing for a stable and efficient streaming service. You will get over than 4000 HD/SD/FHD channels of all the categories you want for yourself and for your family: Movies, series, documentaries, mangas, cartoons, sports….</p> <p>iptv free trial package is compatible with all your devices (Smart TV ‘Samsung, LG, Mag ’250,254 Android…). While installing our Free iptv you will enjoy a wide choice of American channels (Fox HD, Discovery HD, CBS HD, NBA TV…), French channels (Canal +, M6, OCS …), Arab (Bein Sport, MBC …) And all the international channels to enjoy all the box office movies, new documentaries and Netflix serials.<br/> You can consult the list of our channels on this channel page: <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>IPTV</strong></a> channel lists.</p> <p><strong>Buyip-tv.com</strong> usually offer discounts to our customers, mostly on 24 month and 12 month and 6 month subscription.</p> <p>Although we are sure that our <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>IPTV</strong></a> Subscription don’t need any technical knowledge so you won’t face any trouble while the installation. But if for any reason you need support or training to install your service, just contact us via the website or email.<br/> We will be with you at any time of the day!<br/> Get the huge and complete home entertainment experience.</p> <p><strong>All in one place, one website, one click, whatever you love or whenever you are, we’ve got your desire! With our iptv package, you now get more than the exceptive streaming with no issues.</strong></p> <h2>Start your free <a href=\"https://www.buyip-tv.com/free-trial/\">iptv trial</a> Now</h2> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iptvglobal\"> /u/iptvglobal </a> &#32; to &#32; <a href=\"https://www.reddit.com/r/myIPTV/\"> r/myIPTV </a> <br/> <span><a href=\"https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/\">[comments]</a></span>', 'author': '/u/iptvglobal', 'content': [{'language': None, 'value': '<!-- SC_OFF --><div class=\"md\"><h2>1-Day Risk <a href=\"https://www.buyip-tv.com/free-trial/\">Free trial</a>. No obligation, no credit card required. Simply Request <a href=\"https://www.buyip-tv.com/free-trial/\">free</a> <a href=\"https://www.buyip-tv.com/free-trial/\">IPTV Trial</a> and you’ll be up and running within few min.</h2> <p>Are you still confused about getting an IPTV subscription? Do you want an <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>iptv test</strong></a> of the service before paying a huge amount?<br/> <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>IPTV 24-h test</strong></a> Subscription is your solution to get a full test of the <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>iptv</strong></a> serivce and to be sure you are subscribing for a stable and efficient streaming service. You will get over than 4000 HD/SD/FHD channels of all the categories you want for yourself and for your family: Movies, series, documentaries, mangas, cartoons, sports….</p> <p>iptv free trial package is compatible with all your devices (Smart TV ‘Samsung, LG, Mag ’250,254 Android…). While installing our Free iptv you will enjoy a wide choice of American channels (Fox HD, Discovery HD, CBS HD, NBA TV…), French channels (Canal +, M6, OCS …), Arab (Bein Sport, MBC …) And all the international channels to enjoy all the box office movies, new documentaries and Netflix serials.<br/> You can consult the list of our channels on this channel page: <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>IPTV</strong></a> channel lists.</p> <p><strong>Buyip-tv.com</strong> usually offer discounts to our customers, mostly on 24 month and 12 month and 6 month subscription.</p> <p>Although we are sure that our <a href=\"https://www.buyip-tv.com/free-trial/\"><strong>IPTV</strong></a> Subscription don’t need any technical knowledge so you won’t face any trouble while the installation. But if for any reason you need support or training to install your service, just contact us via the website or email.<br/> We will be with you at any time of the day!<br/> Get the huge and complete home entertainment experience.</p> <p><strong>All in one place, one website, one click, whatever you love or whenever you are, we’ve got your desire! With our iptv package, you now get more than the exceptive streaming with no issues.</strong></p> <h2>Start your free <a href=\"https://www.buyip-tv.com/free-trial/\">iptv trial</a> Now</h2> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/iptvglobal\"> /u/iptvglobal </a> &#32; to &#32; <a href=\"https://www.reddit.com/r/myIPTV/\"> r/myIPTV </a> <br/> <span><a href=\"https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/\">[comments]</a></span>', 'base': 'https://www.reddit.com/new/.rss?sort=new', 'type': 'text/html'}], 'link': 'https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/', 'updated': '2018-11-09T10:42:37+00:00', 'href': 'https://www.reddit.com/user/iptvglobal', 'author_detail': {'href': 'https://www.reddit.com/user/iptvglobal', 'name': '/u/iptvglobal'}, 'links': [{'href': 'https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/', 'rel': 'alternate', 'type': 'text/html'}], 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=11, tm_mday=9, tm_hour=10, tm_min=42, tm_sec=37, tm_wday=4, tm_yday=313, tm_isdst=0), 'id': 'https://www.reddit.com/new/t3_9vjflw', 'title_detail': {'language': None, 'value': 'BUYIP-TV.COM Offer iptv trial', 'base': 'https://www.reddit.com/new/.rss?sort=new', 'type': 'text/plain'}}\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "# Define URL of the RSS Feed I want\n",
    "a_reddit_rss_url = 'http://www.reddit.com/new/.rss?sort=new'\n",
    "\n",
    "feed = feedparser.parse( a_reddit_rss_url )\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        print(item)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Reddits\n",
    "\n",
    "As described above, sub-reddits are communities organized around particular topics.\n",
    "\n",
    "Some example sub-reddits:\n",
    " * https://www.reddit.com/r/steak/\n",
    " * https://www.reddit.com/r/datascience/\n",
    " * https://www.reddit.com/r/MachineLearning/\n",
    " * https://www.reddit.com/r/deeplearning/\n",
    " * https://www.reddit.com/r/Python/\n",
    " * https://www.reddit.com/r/Databases/\n",
    " * https://www.reddit.com/r/NoSQL/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Tasks\n",
    " 1. Review the data in an RSS item.\n",
    " 1. Conceptual a database design that can collect the data.\n",
    "    * Make sure you capture at least Author, Tags, Title, Link\n",
    "    * Make sure your items (posts) are unique and not duplicated!\n",
    " 1. Implement the database in your PostgreSQL schema\n",
    " 1. Implement a cell of Python Code that collects the latest post from front page (/new/) and 5-10 sub-reddits (r/.../), then inserts the data into your database.\n",
    " 1. After you have loaded a few hundered posts (items) from the RSS feeds, write an **interesting query** that requires a join across your two or more of your tables.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code to extract some data from the feed items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Title: ASP.NET MVC with Angular or Vue? (https://www.reddit.com/r/webdev/comments/9vjfm5/aspnet_mvc_with_angular_or_vue/)\n",
      "Timestamp: 2018-11-09T10:42:41+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "Hello there.  Been working on a C# Asp.NET backend for a while. Any recommendations for a frontend technology? Doesn't have to be either of the two.  Thanks everyone in advance.  /u/ppallo r/webdev [link] [comments]\n",
      "====================\n",
      "Title: [CN] The Great Ruler - Chapters 782 - 785 (https://www.reddit.com/r/noveltranslations/comments/9vjfm1/cn_the_great_ruler_chapters_782_785/)\n",
      "Timestamp: 2018-11-09T10:42:39+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "The Great Ruler | Da Zhu Zai | 大主宰  Author: Tian Can Tu Dou | 天蚕土豆   Chapter 782  Chapter 783  Chapter 784  Chapter 785   Official Synopsis  The Great Thousand World. It is a place where numerous planes intersect, a place where many clans live and a place where a group of lords assemble. The Heavenly Sovereigns appear one by one from the Lower Planes and they will all display a legend that others would desire as they pursue the road of being a ruler in this boundless world.  In the Endless Fire Territory that the Flame Emperor controls, thousands of fire blazes through the heavens.  Inside the Martial Realm, the power of the Martial Ancestor frightens the heaven and the earth.  At the West Heaven Temple, the might of the Emperor of a Hundred Battles is absolute.  In the Northern Desolate Hill, a place filled with thousands of graves, the Immortal Owner rules the world.  A boy from the Northern Spiritual Realm comes out, riding on a Nine Netherworld Bird, as he charges into the brilliant and diverse world. Just who can rule over their destiny of their path on becoming a Great Ruler? In the Great Thousand World, many strive to become a Great Ruler.   More Links   Table of Contents  Manga Updates  Novel Updates  Raw Source  Previous Thread Search    Translated by /u/Thyaeria at Wuxiaworld  /u/sabret00the r/noveltranslations [link] [comments]\n",
      "====================\n",
      "Title: BUYIP-TV.COM Offer iptv trial (https://www.reddit.com/r/myIPTV/comments/9vjflw/buyiptvcom_offer_iptv_trial/)\n",
      "Timestamp: 2018-11-09T10:42:37+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "1-Day Risk Free trial . No obligation, no credit card required. Simply Request free  IPTV Trial and you’ll be up and running within few min.  Are you still confused about getting an IPTV subscription? Do you want an iptv test of the service before paying a huge amount?  IPTV 24-h test Subscription is your solution to get a full test of the iptv serivce and to be sure you are subscribing for a stable and efficient streaming service. You will get over than 4000 HD/SD/FHD channels of all the categories you want for yourself and for your family: Movies, series, documentaries, mangas, cartoons, sports….  iptv free trial package is compatible with all your devices (Smart TV ‘Samsung, LG, Mag ’250,254 Android…). While installing our Free iptv you will enjoy a wide choice of American channels (Fox HD, Discovery HD, CBS HD, NBA TV…), French channels (Canal +, M6, OCS …), Arab (Bein Sport, MBC …) And all the international channels to enjoy all the box office movies, new documentaries and Netflix serials. You can consult the list of our channels on this channel page: IPTV channel lists.  Buyip-tv.com usually offer discounts to our customers, mostly on 24 month and 12 month and 6 month subscription.  Although we are sure that our IPTV Subscription don’t need any technical knowledge so you won’t face any trouble while the installation. But if for any reason you need support or training to install your service, just contact us via the website or email. We will be with you at any time of the day! Get the huge and complete home entertainment experience.  All in one place, one website, one click, whatever you love or whenever you are, we’ve got your desire! With our iptv package, you now get more than the exceptive streaming with no issues.  Start your free iptv trial Now  /u/iptvglobal r/myIPTV [link] [comments]\n",
      "====================\n",
      "Title: Kensei doing Orochi's executions with Orochi's katana (https://www.reddit.com/r/ForHonorSamurai/comments/9vjfbo/kensei_doing_orochis_executions_with_orochis/)\n",
      "Timestamp: 2018-11-09T10:42:37+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/IrateTeitoku to r/ForHonorSamurai   [link]  [comments] \n",
      "====================\n",
      "Title: Can anybody help me to cheer up??:( (https://www.reddit.com/r/askgaybros/comments/9vjflu/can_anybody_help_me_to_cheer_up/)\n",
      "Timestamp: 2018-11-09T10:42:36+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "Im 28, gay and fell in love with my close friend, classic right? All i wanted is a good friend but i fell for him instead :( he has a gf right now, i used to stop by his desk (we are coworker) everyday after work and he seems so happy whenever i visit him and we go home together, But lately i never did anymore bcs i dont want to be too close with him anymore, yes it feels so iritating, i dont know what he feels about me being like this though :( And today i saw him come home earlier than me, i dont know but i feel so depressed, he usually never come home so early, maybe he had an appointment or maybe another date with his gf :( I spent most of my weekends alone and nobody knows im gay, i live in a conservative country in asia I feel so empty and everyone is leaving me behind :(  /u/xucai r/askgaybros [link] [comments]\n",
      "====================\n",
      "Title: Trying to reignite short lived spark on date tonight - need advice! (https://www.reddit.com/r/relationship_advice/comments/9vjflt/trying_to_reignite_short_lived_spark_on_date/)\n",
      "Timestamp: 2018-11-09T10:42:36+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "I met my (28F) boyfriend (29M) only four months ago and we've been officially together for about two months. We get along amazingly well, our relationship just seems to flow well and we have a great time together. Up until recently the physical side of things was amazing and very frequent but things changed about a week ago. He'd been sick for the week so was really tired and not very talkative which is obviously understandable and I he's allowed to be grumpy if he's sick, it didn't bother me, When he was almost better, I went over to his place but things still seemed a bit off. When I brought it up, he said that he'd been having fears that he would never fall in love with me. I don't think we've known each other long enough to fall properly in love yet but this was still a bit hard to hear as things have been going so well and up until he got sick, I could just feel how much he was into me. I am his first girlfriend and regular sexual partner so he admitted that he has no idea what he should be feeling at this stage and when we talked about it, we wondered if the initial excitement of dating someone new had just faded a bit. I know this is also a normal stage but it does feel a little early for the magic to be fading but I don't think the duration of that magic should matter as I think that's because we grew comfortable and so open with each other so quickly. We talked about it and decided we didn't want to break up and he said he thought his fears of never falling in love with me were probably just his anxiety about relationships. Things have slowly started to feel back to normal over this week, we spent the weekend together with his friends and my family and things still feel very committed, but the physical side if things hasn't come back yet. We didn't see each other during the week which is I think what we needed but our communication on text has felt pretty normal. We have a date tonight (which is actually our first proper one just the two of us in about 6 weeks!) and I guess I am a little bit anxious as to how it's going to go. I don't want to put pressure on it or anything but I want things to feel normal again. We're putting in conscious effort to try and move through this little blip and I'm really hoping there will be some progress in the physical side. Does anyone have advice for this date tonight?  /u/throwaway67583120 r/relationship_advice [link] [comments]\n",
      "====================\n",
      "Title: Supreme Court justice Ginsburg 'up and working' after fall (https://www.reddit.com/r/politics/comments/9vjflq/supreme_court_justice_ginsburg_up_and_working/)\n",
      "Timestamp: 2018-11-09T10:42:35+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "/u/alfosn r/politics [link] [comments]\n",
      "====================\n",
      "Title: Subscription recommendations (https://www.reddit.com/r/ukcigars/comments/9vjflp/subscription_recommendations/)\n",
      "Timestamp: 2018-11-09T10:42:34+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "Are there any cigar subscriptions online that people may recommend? I would like to try a bit of variety without breaking the bank. Cheers  /u/SmartWinston r/ukcigars [link] [comments]\n",
      "====================\n",
      "Title: Perfect (https://www.reddit.com/r/wholesomememes/comments/9vjfln/perfect/)\n",
      "Timestamp: 2018-11-09T10:42:34+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/Yago_R_G to r/wholesomememes   [link]  [comments] \n",
      "====================\n",
      "Title: Everest Announces Partnership with Indonesian Government to Help Track Payouts (https://www.reddit.com/r/CryptoNews/comments/9vjfll/everest_announces_partnership_with_indonesian/)\n",
      "Timestamp: 2018-11-09T10:42:33+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/Stiffywantsjitty to r/CryptoNews   [link]  [comments] \n",
      "====================\n",
      "Title: i had a weird game (https://www.reddit.com/r/pkmntcg/comments/9vjflj/i_had_a_weird_game/)\n",
      "Timestamp: 2018-11-09T10:42:33+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "opponent only played a yungoose and did not attach any energy to it and his/her hand had more then 10 cards ,he/she kept skipping turns  /u/Evilbefalls r/pkmntcg [link] [comments]\n",
      "====================\n",
      "Title: When you're young everyone is telling you to grow up, but when you do, you realize everyone is trying to stay young (https://www.reddit.com/r/Showerthoughts/comments/9vjflh/when_youre_young_everyone_is_telling_you_to_grow/)\n",
      "Timestamp: 2018-11-09T10:42:33+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "/u/HashDaWook r/Showerthoughts [link] [comments]\n",
      "====================\n",
      "Title: Ubuntu startup shared library access (https://www.reddit.com/r/linuxquestions/comments/9vjfle/ubuntu_startup_shared_library_access/)\n",
      "Timestamp: 2018-11-09T10:42:32+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "I'm currently trying to find a way that which processors are using which shared libraries during boot up time.  I have tried to do LD_PRELOAD way but, it wasn't successful.  ​  Is there a way actually gather information while boot up?  /u/ekstrah r/linuxquestions [link] [comments]\n",
      "====================\n",
      "Title: Real estate agents of reddit what’s the craziest thing you have walked in on? (https://www.reddit.com/r/AskReddit/comments/9vjfla/real_estate_agents_of_reddit_whats_the_craziest/)\n",
      "Timestamp: 2018-11-09T10:42:31+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "/u/Outofhereatnoon r/AskReddit [link] [comments]\n",
      "====================\n",
      "Title: Vols moins cher de San Jose vers (https://www.reddit.com/r/Voyage/comments/9vjfl9/vols_moins_cher_de_san_jose_vers/)\n",
      "Timestamp: 2018-11-09T10:42:31+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "/u/voyagecost r/Voyage [link] [comments]\n",
      "====================\n",
      "Title: Wait... Griffin is not breedable?!?!? (https://www.reddit.com/r/playarkmobile/comments/9vjfl7/wait_griffin_is_not_breedable/)\n",
      "Timestamp: 2018-11-09T10:42:30+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "/u/STAZEZ r/playarkmobile [link] [comments]\n",
      "====================\n",
      "Title: No lucky chloe headset replica? (https://www.reddit.com/r/Tekken/comments/9vjfl6/no_lucky_chloe_headset_replica/)\n",
      "Timestamp: 2018-11-09T10:42:30+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "I am literally surprised that there is no such a thing like replica of Lucky Chloe's headset from game. Or maybe I cant use google properly?  /u/LoraKenkech r/Tekken [link] [comments]\n",
      "====================\n",
      "Title: We've made a new Bitcoin mixer [Zatoshi Company] /r/Bitcoin (https://www.reddit.com/r/BitcoinAll/comments/9vjfl1/weve_made_a_new_bitcoin_mixer_zatoshi_company/)\n",
      "Timestamp: 2018-11-09T10:42:29+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/ABitcoinAllBot to r/BitcoinAll   [link]  [comments] \n",
      "====================\n",
      "Title: レオパレス赤字転落 アパート施工不良響く 窮するレオパレス (https://www.reddit.com/r/newsokunomoral/comments/9vjfl0/レオパレス赤字転落_アパート施工不良響く_窮するレオパレス/)\n",
      "Timestamp: 2018-11-09T10:42:28+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/EWI3000 to r/newsokunomoral   [link]  [comments] \n",
      "====================\n",
      "Title: Usa, la statistica agghiacciante dei morti da armi da fuoco (https://www.reddit.com/r/Italia/comments/9vjfkz/usa_la_statistica_agghiacciante_dei_morti_da_armi/)\n",
      "Timestamp: 2018-11-09T10:42:28+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/Lucaspo67 to r/Italia   [link]  [comments] \n",
      "====================\n",
      "Title: Wacom Cintiq Black Friday Discounts of 10-25% Predicted Depending on Model [Amazon] (https://www.reddit.com/r/BlackFridayTech/comments/9vjfky/wacom_cintiq_black_friday_discounts_of_1025/)\n",
      "Timestamp: 2018-11-09T10:42:28+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/asburyw to r/BlackFridayTech   [link]  [comments] \n",
      "====================\n",
      "Title: Aditi Rao Hydari in the perfect position (https://www.reddit.com/r/HQDesi/comments/9vjfkw/aditi_rao_hydari_in_the_perfect_position/)\n",
      "Timestamp: 2018-11-09T10:42:28+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "     submitted by /u/phdinbakchodi to r/HQDesi   [link]  [comments] \n",
      "====================\n",
      "Title: Karatcoin offers a decentralized loan product! (https://www.reddit.com/r/Crypto_ico/comments/9vjfku/karatcoin_offers_a_decentralized_loan_product/)\n",
      "Timestamp: 2018-11-09T10:42:27+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "      ​  https://i.redd.it/delu16yqaax11.png  Karatcoin offers a decentralized loan product for eligible members, known as the Karatcoin Loan (KCL) through a smart contract issuing KCX with a financial term that are tailored to the needs of the purpose of the financing or the member country’s overall debt management strategy. #KARATCOIN #KCD https://karatcoin.co   submitted by /u/irida888 to r/Crypto_ico   [link]  [comments] \n",
      "====================\n",
      "Title: Is it possible to create this calendar based reminder shortcut? (https://www.reddit.com/r/shortcuts/comments/9vjfkr/is_it_possible_to_create_this_calendar_based/)\n",
      "Timestamp: 2018-11-09T10:42:27+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "Hi guys,  I am looking to create a shortcut that will automatically create multiple reminders at the end of a working day (my shift pattern is 4 on 4 off, which is in my calendar) depending in if I am on shift or not that day. Is this even possible or should I stop trying?  /u/Skyfluks r/shortcuts [link] [comments]\n",
      "====================\n",
      "Title: We've made a new Bitcoin mixer [Zatoshi Company] (https://www.reddit.com/r/Bitcoin/comments/9vjfko/weve_made_a_new_bitcoin_mixer_zatoshi_company/)\n",
      "Timestamp: 2018-11-09T10:42:26+00:00\n",
      "--------------------\n",
      "Summary:\n",
      "Hello All!  Our team just finished a new mixer based on the CoinJoin method and I am pretty sure that we did a good job to combine it with our personal algorithms.  I am proud to announce that it’s live now! Try it out and tell us what you think, we’re always happy to chat and receive your feedback.  Also, you can visit our website, we are providing different type of solutions related to cryptocurrency integration into business.  You can use this link https://zatoshi.com/   /u/Zatoshi_official r/Bitcoin [link] [comments]\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "# Functions from: https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "# Define URL of the RSS Feed I want\n",
    "a_reddit_rss_url = 'http://www.reddit.com/new/.rss?sort=new'\n",
    "\n",
    "feed = feedparser.parse( a_reddit_rss_url )\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        dttm = item[ \"date\" ]\n",
    "        title = item[ \"title\" ]\n",
    "        summary_text = text_from_html(item[ \"summary\" ])\n",
    "        link = item[ \"link\" ]\n",
    "        \n",
    "        print(\"====================\")\n",
    "        print(\"Title: {} ({})\\nTimestamp: {}\".format(title,link,dttm))\n",
    "        print(\"--------------------\\nSummary:\\n{}\".format(summary_text))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3:E2:Q1 - Task 1: Annotate your Entities and their attributes\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Your annotations and analysis \n",
    "##  in this cell\n",
    "## ------------------------\n",
    "\n",
    "reddit_author:\n",
    "    author\n",
    "    tag\n",
    "\n",
    "reddit_posts:\n",
    "    author\n",
    "    title\n",
    "    link\n",
    "    time\n",
    "    summary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3:E2:Q2 -  Task 2: Implement the database in your PostgreSQL schema\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Your DDL SQL in this cell\n",
    "## -------------------------\n",
    "\n",
    "\n",
    "CREATE TABLE dlfy6.reddit_author(\n",
    "    author TEXT PRIMARY KEY,\n",
    "    tag TEXT\n",
    ");\n",
    "  \n",
    "\n",
    "\n",
    "CREATE TABLE dlfy6.reddit_posts(\n",
    "    author TEXT,\n",
    "    title TEXT,\n",
    "    link TEXT,\n",
    "    time TIMESTAMP,\n",
    "    summary TEXT,\n",
    "    FOREIGN KEY (author)\n",
    "        REFERENCES reddit_author(author)\n",
    ");\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3:E2:Q3 - Task 3: Python Code to collect RSS data from Reddit\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Write the steps you would take to do this. \n",
    "\n",
    "- Collect data from url\n",
    "- Collect title\n",
    "- Collect date\n",
    "- Collect link\n",
    "- Collect tags\n",
    "- Collect author\n",
    "- Collect summary\n",
    "- Create dictionary to add data\n",
    "- upload data to database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement your answer in this cell\n",
    "## ------------------------\n",
    "import pandas as pd\n",
    "a_reddit_rss_url = 'https://www.reddit.com/.rss?sort=new&limit=100'\n",
    "\n",
    "feed = feedparser.parse( a_reddit_rss_url )\n",
    "reddit_author = pd.DataFrame(columns=[['author','tag']])\n",
    "reddit_posts = pd.DataFrame(columns=[['author','title','link','time','summary']])\n",
    "\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        author = item[\"author\"]\n",
    "        for a in item[\"tags\"]:\n",
    "            reddit_author = reddit_author.append({'author': author ,'tag': a[ \"label\" ]}, ignore_index=True)        \n",
    "        \n",
    "        reddit_posts = reddit_posts.append({'author': item[ \"author\" ],'title': item[ \"title\" ],'link': item[ \"link\" ],'time': item[ \"date\" ],'summary': text_from_html(item[ \"summary\"]) }, ignore_index=True)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      "author    100 non-null object\n",
      "tag       100 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datascience_url = 'https://www.reddit.com/r/datascience/.rss?sort=new&limit=100'\n",
    "\n",
    "feed = feedparser.parse( datascience_url )\n",
    "\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        author = item[\"author\"]\n",
    "        for a in item[\"tags\"]:\n",
    "            reddit_author = reddit_author.append({'author': author ,'tag': a[ \"label\" ]}, ignore_index=True)        \n",
    "        \n",
    "        reddit_posts = reddit_posts.append({'author': item[ \"author\" ],'title': item[ \"title\" ],'link': item[ \"link\" ],'time': item[ \"date\" ],'summary': text_from_html(item[ \"summary\"]) }, ignore_index=True)        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 202 entries, 0 to 201\n",
      "Data columns (total 2 columns):\n",
      "author    202 non-null object\n",
      "tag       202 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "machinelearning_url = 'https://www.reddit.com/r/MachineLearning/.rss?sort=new&limit=100'\n",
    "\n",
    "feed = feedparser.parse( machinelearning_url )\n",
    "\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        author = item[\"author\"]\n",
    "        for a in item[\"tags\"]:\n",
    "            reddit_author = reddit_author.append({'author': author ,'tag': a[ \"label\" ]}, ignore_index=True)        \n",
    "        \n",
    "        reddit_posts = reddit_posts.append({'author': item[ \"author\" ],'title': item[ \"title\" ],'link': item[ \"link\" ],'time': item[ \"date\" ],'summary': text_from_html(item[ \"summary\"]) }, ignore_index=True)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 304 entries, 0 to 303\n",
      "Data columns (total 2 columns):\n",
      "author    304 non-null object\n",
      "tag       304 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplearning_url = 'https://www.reddit.com/r/deeplearning/.rss?sort=new&limit=100'\n",
    "\n",
    "feed = feedparser.parse( deeplearning_url )\n",
    "\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        author = item[\"author\"]\n",
    "        for a in item[\"tags\"]:\n",
    "            reddit_author = reddit_author.append({'author': author ,'tag': a[ \"label\" ]}, ignore_index=True)        \n",
    "        \n",
    "        reddit_posts = reddit_posts.append({'author': item[ \"author\" ],'title': item[ \"title\" ],'link': item[ \"link\" ],'time': item[ \"date\" ],'summary': text_from_html(item[ \"summary\"]) }, ignore_index=True)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_url = 'https://www.reddit.com/r/Python/.rss?sort=new&limit=100'\n",
    "\n",
    "feed = feedparser.parse( python_url )\n",
    "\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        author = item[\"author\"]\n",
    "        for a in item[\"tags\"]:\n",
    "            reddit_author = reddit_author.append({'author': author ,'tag': a[ \"label\" ]}, ignore_index=True)        \n",
    "        \n",
    "        reddit_posts = reddit_posts.append({'author': item[ \"author\" ],'title': item[ \"title\" ],'link': item[ \"link\" ],'time': item[ \"date\" ],'summary': text_from_html(item[ \"summary\"]) }, ignore_index=True)        \n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_url = 'https://www.reddit.com/r/NoSQL/.rss?sort=new&limit=100'\n",
    "\n",
    "feed = feedparser.parse( sql_url )\n",
    "\n",
    "\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        author = item[\"author\"]\n",
    "        for a in item[\"tags\"]:\n",
    "            reddit_author = reddit_author.append({'author': author ,'tag': a[ \"label\" ]}, ignore_index=True)        \n",
    "        \n",
    "        reddit_posts = reddit_posts.append({'author': item[ \"author\" ],'title': item[ \"title\" ],'link': item[ \"link\" ],'time': item[ \"date\" ],'summary': text_from_html(item[ \"summary\"]) }, ignore_index=True)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/u/clgmae104</td>\n",
       "      <td>r/whatisthisthing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/u/amco3008</td>\n",
       "      <td>r/AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/u/adviseme3737</td>\n",
       "      <td>r/legaladvice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/u/_Caketaco_</td>\n",
       "      <td>r/copypasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/u/Pixelcitizen98</td>\n",
       "      <td>r/OutOfTheLoop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                tag\n",
       "0       /u/clgmae104  r/whatisthisthing\n",
       "1        /u/amco3008        r/AskReddit\n",
       "2    /u/adviseme3737      r/legaladvice\n",
       "3      /u/_Caketaco_        r/copypasta\n",
       "4  /u/Pixelcitizen98     r/OutOfTheLoop"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 606 entries, 0 to 605\n",
      "Data columns (total 2 columns):\n",
      "author    606 non-null object\n",
      "tag       606 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    535\n",
       "tag       100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_author.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "reddit_author=reddit_author.drop_duplicates(['author'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 535 entries, 0 to 605\n",
      "Data columns (total 2 columns):\n",
      "author    535 non-null object\n",
      "tag       535 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 12.5+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_author=reddit_author.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 535 entries, 0 to 605\n",
      "Data columns (total 2 columns):\n",
      "author    535 non-null object\n",
      "tag       535 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 12.5+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    0\n",
       "tag       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_author.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author    535\n",
       "tag        97\n",
       "dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_author.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>time</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/u/GeneLatifah</td>\n",
       "      <td>Gillum responds to Scott lawsuit: ‘Counting vo...</td>\n",
       "      <td>https://www.reddit.com/r/politics/comments/9vk...</td>\n",
       "      <td>2018-11-09T14:14:55+00:00</td>\n",
       "      <td>submitted by /u/GeneLatifah to r/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/u/mvea</td>\n",
       "      <td>US cigarette smoking rate reaches new low - Ci...</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/9vkc...</td>\n",
       "      <td>2018-11-09T13:18:23+00:00</td>\n",
       "      <td>submitted by /u/mvea to r/science   [link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/u/clgmae104</td>\n",
       "      <td>What is this rodent that just climbed out of m...</td>\n",
       "      <td>https://www.reddit.com/r/whatisthisthing/comme...</td>\n",
       "      <td>2018-11-09T01:52:21+00:00</td>\n",
       "      <td>submitted by /u/clgmae104 to r/whatisthis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/u/EverythingTittysBoii</td>\n",
       "      <td>Gave him a forever home yesterday and thought ...</td>\n",
       "      <td>https://www.reddit.com/r/aww/comments/9vkcqy/g...</td>\n",
       "      <td>2018-11-09T13:20:19+00:00</td>\n",
       "      <td>submitted by /u/EverythingTittysBoii to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/u/evgat2</td>\n",
       "      <td>The frosting on my car window was melted by th...</td>\n",
       "      <td>https://www.reddit.com/r/mildlyinteresting/com...</td>\n",
       "      <td>2018-11-09T12:48:22+00:00</td>\n",
       "      <td>submitted by /u/evgat2 to r/mildlyinteres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author                                              title  \\\n",
       "0           /u/GeneLatifah  Gillum responds to Scott lawsuit: ‘Counting vo...   \n",
       "1                  /u/mvea  US cigarette smoking rate reaches new low - Ci...   \n",
       "2             /u/clgmae104  What is this rodent that just climbed out of m...   \n",
       "3  /u/EverythingTittysBoii  Gave him a forever home yesterday and thought ...   \n",
       "4                /u/evgat2  The frosting on my car window was melted by th...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.reddit.com/r/politics/comments/9vk...   \n",
       "1  https://www.reddit.com/r/science/comments/9vkc...   \n",
       "2  https://www.reddit.com/r/whatisthisthing/comme...   \n",
       "3  https://www.reddit.com/r/aww/comments/9vkcqy/g...   \n",
       "4  https://www.reddit.com/r/mildlyinteresting/com...   \n",
       "\n",
       "                        time  \\\n",
       "0  2018-11-09T14:14:55+00:00   \n",
       "1  2018-11-09T13:18:23+00:00   \n",
       "2  2018-11-09T01:52:21+00:00   \n",
       "3  2018-11-09T13:20:19+00:00   \n",
       "4  2018-11-09T12:48:22+00:00   \n",
       "\n",
       "                                             summary  \n",
       "0       submitted by /u/GeneLatifah to r/politics...  \n",
       "1       submitted by /u/mvea to r/science   [link...  \n",
       "2       submitted by /u/clgmae104 to r/whatisthis...  \n",
       "3       submitted by /u/EverythingTittysBoii to r...  \n",
       "4       submitted by /u/evgat2 to r/mildlyinteres...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 606 entries, 0 to 605\n",
      "Data columns (total 5 columns):\n",
      "author     606 non-null object\n",
      "title      606 non-null object\n",
      "link       606 non-null object\n",
      "time       606 non-null object\n",
      "summary    606 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 23.8+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_posts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_post= reddit_posts.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_posts=reddit_posts.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 606 entries, 0 to 605\n",
      "Data columns (total 5 columns):\n",
      "author     606 non-null object\n",
      "title      606 non-null object\n",
      "link       606 non-null object\n",
      "time       606 non-null object\n",
      "summary    606 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 28.4+ KB\n"
     ]
    }
   ],
   "source": [
    "reddit_posts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "# This collects a masked password from the user\n",
    "mypasswd = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "from psycopg2.extensions import adapt, register_adapter, AsIs\n",
    "\n",
    "# Then connects to the DB\n",
    "connection = psycopg2.connect(database = 'dsa_student', \n",
    "                              user = 'dlfy6', \n",
    "                              host = 'dbase.dsa.missouri.edu',\n",
    "                              password = mypasswd)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Then remove the password from computer memory\n",
    "del mypasswd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_author = reddit_author.where(pd.notnull(data), None)\n",
    "\n",
    "\n",
    "register_adapter(np.int64,AsIs)\n",
    "register_adapter(np.float64,AsIs)\n",
    "\n",
    "for row in reddit_author.itertuples(index=False, name ='None'):\n",
    "    #print(row)\n",
    "    cursor.execute('rollback;')\n",
    "    cursor.execute('INSERT INTO dlfy6.reddit_author VALUES(%s,%s)',row)\n",
    "    \n",
    "    \n",
    "# Save (commit) the changes\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reddit_posts = reddit_posts.where(pd.notnull(data), None)\n",
    "\n",
    "register_adapter(np.int64,AsIs)\n",
    "register_adapter(np.float64,AsIs)\n",
    "\n",
    "    \n",
    "for row in reddit_posts.itertuples(index=False, name ='None'):\n",
    "    #print(row)\n",
    "    cursor.execute('rollback;')\n",
    "    cursor.execute('INSERT INTO dlfy6.reddit_posts VALUES(%s,%s,%s,%s,%s)',row)\n",
    "    \n",
    "    \n",
    "# Save (commit) the changes\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3:E2:Q4 - Task 4: Your interesting query of your data\n",
    "**Feel free to add additional cells and write extra queries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL =\"\"\"\n",
    "\n",
    "SELECT a.author, a.tag, p.link\n",
    "FROM dlfy6.reddit_posts p\n",
    "JOIN dlfy6.reddit_author a\n",
    "USING (author);\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute(SQL)\n",
    "    df = cursor.fetchall()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>tag</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/u/clgmae104</td>\n",
       "      <td>r/whatisthisthing</td>\n",
       "      <td>https://www.reddit.com/r/whatisthisthing/comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/u/amco3008</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>https://www.reddit.com/r/AskReddit/comments/9v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/u/adviseme3737</td>\n",
       "      <td>r/legaladvice</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/u/_Caketaco_</td>\n",
       "      <td>r/copypasta</td>\n",
       "      <td>https://www.reddit.com/r/copypasta/comments/9v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/u/Pixelcitizen98</td>\n",
       "      <td>r/OutOfTheLoop</td>\n",
       "      <td>https://www.reddit.com/r/OutOfTheLoop/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/u/GeneLatifah</td>\n",
       "      <td>r/politics</td>\n",
       "      <td>https://www.reddit.com/r/politics/comments/9vk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/u/screaming_librarian</td>\n",
       "      <td>r/news</td>\n",
       "      <td>https://www.reddit.com/r/news/comments/9vhpmv/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/u/syd430</td>\n",
       "      <td>r/TopMindsOfReddit</td>\n",
       "      <td>https://www.reddit.com/r/TopMindsOfReddit/comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/u/mvea</td>\n",
       "      <td>r/science</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/9vkc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/u/EverythingTittysBoii</td>\n",
       "      <td>r/aww</td>\n",
       "      <td>https://www.reddit.com/r/aww/comments/9vkcqy/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/u/evgat2</td>\n",
       "      <td>r/mildlyinteresting</td>\n",
       "      <td>https://www.reddit.com/r/mildlyinteresting/com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/u/Justsoinsane</td>\n",
       "      <td>r/MovieDetails</td>\n",
       "      <td>https://www.reddit.com/r/MovieDetails/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/u/FederalParsley2</td>\n",
       "      <td>r/relationship_advice</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/u/gamzob</td>\n",
       "      <td>r/worldnews</td>\n",
       "      <td>https://www.reddit.com/r/worldnews/comments/9v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/u/EviscerationNation</td>\n",
       "      <td>r/interestingasfuck</td>\n",
       "      <td>https://www.reddit.com/r/WhitePeopleTwitter/co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/u/nerdmor</td>\n",
       "      <td>r/MaliciousCompliance</td>\n",
       "      <td>https://www.reddit.com/r/MaliciousCompliance/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/u/NubOnReddit</td>\n",
       "      <td>r/marvelstudios</td>\n",
       "      <td>https://www.reddit.com/r/marvelstudios/comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/u/MashedPotatoh</td>\n",
       "      <td>r/gaming</td>\n",
       "      <td>https://www.reddit.com/r/gaming/comments/9vjxj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/u/GallowBoob</td>\n",
       "      <td>r/facepalm</td>\n",
       "      <td>https://www.reddit.com/r/pics/comments/9vjn8o/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/u/mafyoo</td>\n",
       "      <td>r/funny</td>\n",
       "      <td>https://www.reddit.com/r/funny/comments/9vjlkh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/u/Valerokai</td>\n",
       "      <td>r/fakehistoryporn</td>\n",
       "      <td>https://www.reddit.com/r/fakehistoryporn/comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/u/IGotTheShit</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/u/motleyblondie</td>\n",
       "      <td>r/sysadmin</td>\n",
       "      <td>https://www.reddit.com/r/sysadmin/comments/9vf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/u/myoclonicdork</td>\n",
       "      <td>r/AskMen</td>\n",
       "      <td>https://www.reddit.com/r/AskMen/comments/9vg2x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/u/icant-chooseone</td>\n",
       "      <td>r/Unexpected</td>\n",
       "      <td>https://www.reddit.com/r/Unexpected/comments/9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/u/nocte_lupus</td>\n",
       "      <td>r/britishproblems</td>\n",
       "      <td>https://www.reddit.com/r/britishproblems/comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/u/Lugeau</td>\n",
       "      <td>r/2healthbars</td>\n",
       "      <td>https://www.reddit.com/r/2healthbars/comments/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/u/BubbaJoeJones</td>\n",
       "      <td>r/UnresolvedMysteries</td>\n",
       "      <td>https://www.reddit.com/r/UnresolvedMysteries/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/u/Portis403</td>\n",
       "      <td>r/space</td>\n",
       "      <td>https://www.reddit.com/r/space/comments/9vk7fh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/u/Caryatid4693</td>\n",
       "      <td>r/femalefashionadvice</td>\n",
       "      <td>https://www.reddit.com/r/femalefashionadvice/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6yonel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>/u/hyc_symas</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6x9a5h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6x82y2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>/u/lengthy_preamble</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6ue2vf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>/u/darkkingll</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6tl3xi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6qzvoz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>/u/RubiksCodeNMZ</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6qvmuh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>/u/RubiksCodeNMZ</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6p7qdr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>/u/RubiksCodeNMZ</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6ofknd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6nu1yg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>/u/Worse_Username</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6n8auq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>/u/rvncerr</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6n3kzw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>/u/rcardin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6mu6k1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>/u/darexinfinity</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6mr2ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>/u/fhoffa</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6mgc9p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>/u/buffyoda</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6k4d5j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6jzm21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>/u/brazorf</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6jb2nw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>/u/mav_918</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6h4hrl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>/u/mooburger</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6fwfep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>/u/anidotnet</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6fdaz9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>/u/campuscodi</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6etg1b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>/u/trickyanswers</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6em1x7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>/u/inmn</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6eix6p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6c2o86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>/u/fhoffa</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/6blknm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/68znbt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>/u/gar_den</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/68mz6y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>/u/venture68</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/68j7m5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>/u/DennisAnikin</td>\n",
       "      <td>r/nosql</td>\n",
       "      <td>https://www.reddit.com/r/nosql/comments/67nqhl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>606 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      author                    tag  \\\n",
       "0               /u/clgmae104      r/whatisthisthing   \n",
       "1                /u/amco3008            r/AskReddit   \n",
       "2            /u/adviseme3737          r/legaladvice   \n",
       "3              /u/_Caketaco_            r/copypasta   \n",
       "4          /u/Pixelcitizen98         r/OutOfTheLoop   \n",
       "5             /u/GeneLatifah             r/politics   \n",
       "6     /u/screaming_librarian                 r/news   \n",
       "7                  /u/syd430     r/TopMindsOfReddit   \n",
       "8                    /u/mvea              r/science   \n",
       "9    /u/EverythingTittysBoii                  r/aww   \n",
       "10                 /u/evgat2    r/mildlyinteresting   \n",
       "11           /u/Justsoinsane         r/MovieDetails   \n",
       "12        /u/FederalParsley2  r/relationship_advice   \n",
       "13                 /u/gamzob            r/worldnews   \n",
       "14     /u/EviscerationNation    r/interestingasfuck   \n",
       "15                /u/nerdmor  r/MaliciousCompliance   \n",
       "16            /u/NubOnReddit        r/marvelstudios   \n",
       "17          /u/MashedPotatoh               r/gaming   \n",
       "18             /u/GallowBoob             r/facepalm   \n",
       "19                 /u/mafyoo                r/funny   \n",
       "20              /u/Valerokai      r/fakehistoryporn   \n",
       "21            /u/IGotTheShit        r/AmItheAsshole   \n",
       "22          /u/motleyblondie             r/sysadmin   \n",
       "23          /u/myoclonicdork               r/AskMen   \n",
       "24        /u/icant-chooseone           r/Unexpected   \n",
       "25            /u/nocte_lupus      r/britishproblems   \n",
       "26                 /u/Lugeau          r/2healthbars   \n",
       "27          /u/BubbaJoeJones  r/UnresolvedMysteries   \n",
       "28              /u/Portis403                r/space   \n",
       "29           /u/Caryatid4693  r/femalefashionadvice   \n",
       "..                       ...                    ...   \n",
       "576          /u/DennisAnikin                r/nosql   \n",
       "577             /u/hyc_symas                r/nosql   \n",
       "578          /u/DennisAnikin                r/nosql   \n",
       "579      /u/lengthy_preamble                r/nosql   \n",
       "580            /u/darkkingll                r/nosql   \n",
       "581          /u/DennisAnikin                r/nosql   \n",
       "582         /u/RubiksCodeNMZ                r/nosql   \n",
       "583         /u/RubiksCodeNMZ                r/nosql   \n",
       "584         /u/RubiksCodeNMZ                r/nosql   \n",
       "585          /u/DennisAnikin                r/nosql   \n",
       "586        /u/Worse_Username                r/nosql   \n",
       "587               /u/rvncerr                r/nosql   \n",
       "588               /u/rcardin                r/nosql   \n",
       "589         /u/darexinfinity                r/nosql   \n",
       "590                /u/fhoffa                r/nosql   \n",
       "591              /u/buffyoda                r/nosql   \n",
       "592          /u/DennisAnikin                r/nosql   \n",
       "593               /u/brazorf                r/nosql   \n",
       "594               /u/mav_918                r/nosql   \n",
       "595             /u/mooburger                r/nosql   \n",
       "596             /u/anidotnet                r/nosql   \n",
       "597            /u/campuscodi                r/nosql   \n",
       "598         /u/trickyanswers                r/nosql   \n",
       "599                  /u/inmn                r/nosql   \n",
       "600          /u/DennisAnikin                r/nosql   \n",
       "601                /u/fhoffa                r/nosql   \n",
       "602          /u/DennisAnikin                r/nosql   \n",
       "603               /u/gar_den                r/nosql   \n",
       "604             /u/venture68                r/nosql   \n",
       "605          /u/DennisAnikin                r/nosql   \n",
       "\n",
       "                                                  link  \n",
       "0    https://www.reddit.com/r/whatisthisthing/comme...  \n",
       "1    https://www.reddit.com/r/AskReddit/comments/9v...  \n",
       "2    https://www.reddit.com/r/legaladvice/comments/...  \n",
       "3    https://www.reddit.com/r/copypasta/comments/9v...  \n",
       "4    https://www.reddit.com/r/OutOfTheLoop/comments...  \n",
       "5    https://www.reddit.com/r/politics/comments/9vk...  \n",
       "6    https://www.reddit.com/r/news/comments/9vhpmv/...  \n",
       "7    https://www.reddit.com/r/TopMindsOfReddit/comm...  \n",
       "8    https://www.reddit.com/r/science/comments/9vkc...  \n",
       "9    https://www.reddit.com/r/aww/comments/9vkcqy/g...  \n",
       "10   https://www.reddit.com/r/mildlyinteresting/com...  \n",
       "11   https://www.reddit.com/r/MovieDetails/comments...  \n",
       "12   https://www.reddit.com/r/relationship_advice/c...  \n",
       "13   https://www.reddit.com/r/worldnews/comments/9v...  \n",
       "14   https://www.reddit.com/r/WhitePeopleTwitter/co...  \n",
       "15   https://www.reddit.com/r/MaliciousCompliance/c...  \n",
       "16   https://www.reddit.com/r/marvelstudios/comment...  \n",
       "17   https://www.reddit.com/r/gaming/comments/9vjxj...  \n",
       "18   https://www.reddit.com/r/pics/comments/9vjn8o/...  \n",
       "19   https://www.reddit.com/r/funny/comments/9vjlkh...  \n",
       "20   https://www.reddit.com/r/fakehistoryporn/comme...  \n",
       "21   https://www.reddit.com/r/AmItheAsshole/comment...  \n",
       "22   https://www.reddit.com/r/sysadmin/comments/9vf...  \n",
       "23   https://www.reddit.com/r/AskMen/comments/9vg2x...  \n",
       "24   https://www.reddit.com/r/Unexpected/comments/9...  \n",
       "25   https://www.reddit.com/r/britishproblems/comme...  \n",
       "26   https://www.reddit.com/r/2healthbars/comments/...  \n",
       "27   https://www.reddit.com/r/UnresolvedMysteries/c...  \n",
       "28   https://www.reddit.com/r/space/comments/9vk7fh...  \n",
       "29   https://www.reddit.com/r/femalefashionadvice/c...  \n",
       "..                                                 ...  \n",
       "576  https://www.reddit.com/r/nosql/comments/6yonel...  \n",
       "577  https://www.reddit.com/r/nosql/comments/6x9a5h...  \n",
       "578  https://www.reddit.com/r/nosql/comments/6x82y2...  \n",
       "579  https://www.reddit.com/r/nosql/comments/6ue2vf...  \n",
       "580  https://www.reddit.com/r/nosql/comments/6tl3xi...  \n",
       "581  https://www.reddit.com/r/nosql/comments/6qzvoz...  \n",
       "582  https://www.reddit.com/r/nosql/comments/6qvmuh...  \n",
       "583  https://www.reddit.com/r/nosql/comments/6p7qdr...  \n",
       "584  https://www.reddit.com/r/nosql/comments/6ofknd...  \n",
       "585  https://www.reddit.com/r/nosql/comments/6nu1yg...  \n",
       "586  https://www.reddit.com/r/nosql/comments/6n8auq...  \n",
       "587  https://www.reddit.com/r/nosql/comments/6n3kzw...  \n",
       "588  https://www.reddit.com/r/nosql/comments/6mu6k1...  \n",
       "589  https://www.reddit.com/r/nosql/comments/6mr2ur...  \n",
       "590  https://www.reddit.com/r/nosql/comments/6mgc9p...  \n",
       "591  https://www.reddit.com/r/nosql/comments/6k4d5j...  \n",
       "592  https://www.reddit.com/r/nosql/comments/6jzm21...  \n",
       "593  https://www.reddit.com/r/nosql/comments/6jb2nw...  \n",
       "594  https://www.reddit.com/r/nosql/comments/6h4hrl...  \n",
       "595  https://www.reddit.com/r/nosql/comments/6fwfep...  \n",
       "596  https://www.reddit.com/r/nosql/comments/6fdaz9...  \n",
       "597  https://www.reddit.com/r/nosql/comments/6etg1b...  \n",
       "598  https://www.reddit.com/r/nosql/comments/6em1x7...  \n",
       "599  https://www.reddit.com/r/nosql/comments/6eix6p...  \n",
       "600  https://www.reddit.com/r/nosql/comments/6c2o86...  \n",
       "601  https://www.reddit.com/r/nosql/comments/6blknm...  \n",
       "602  https://www.reddit.com/r/nosql/comments/68znbt...  \n",
       "603  https://www.reddit.com/r/nosql/comments/68mz6y...  \n",
       "604  https://www.reddit.com/r/nosql/comments/68j7m5...  \n",
       "605  https://www.reddit.com/r/nosql/comments/67nqhl...  \n",
       "\n",
       "[606 rows x 3 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(df,columns=['author','tag','link'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I want to see which links are related to each tag.  I connected 2 tables to get my results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
