{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:red;font-weight:bold;background:yellow;text-align:center;padding:10px;border:solid\">\n",
    "    <h1>RUN IN EMR CLUSTER ONLY</h1>\n",
    "    If the URL of the current page does not begin with \"ec2\", then do **NOT** proceed!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PySpark Practice\n",
    "In this practice, you will use the tools you learned in the readings and labs to perform some computation using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = !hostname\n",
    "if \"dsa\" in name[0]:\n",
    "    raise RuntimeError(\"Only run this notebook in the EMR Cluster!\")\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark-lab\")\n",
    "spark_context = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "Bring in the Dataset\n",
    "For this portion of the practice, you will be using a familiar dataset: the Amazon Review Dataset. Bring in the dataset using SparkSQL in the box below\n",
    "\n",
    "(For reference, this dataset is located in `/datasets/amazon_reviews.json`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# To use Spark SQL we create a SQLContext from SparkContext\n",
    "sqlContext = SQLContext(spark_context)\n",
    "\n",
    "# Location of the dataset on HDFS\n",
    "DATASET = '/datasets/amazon_reviews.json'\n",
    "\n",
    "# Load a table with a CSV format reader and get the first 1000 rows\n",
    "df = sqlContext.read.json(DATASET).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(helpfulness_count=7, helpfulness_score=7, price=-1.0, product_id='B000179R3I', profile_name='Jeanmarie Kabala \"JP Kabala\"', score=4.0, summary='Periwinkle Dartmouth Blazer', text='I own the Austin Reed dartmouth blazer in every color in which they make it-- it is a staple of my business wardrobe. Well made, quality fabric, nicely tailored, classic lines, appropriate for a professional woman. (something that can be hard to find at times) It should be noted, however, that the periwinkle and raspberry colors are lovely, but the fabric and buttons are slightly different than the \"classic\" colors(lighter) and the linings and interfacings are not as substantial as the brown, navy, camel, red and ivory. It\\'s still a good value, particularly as these are colors appropriate to warmer seasons and climates, but I was a bit surprised.', time=1182816000, title='Amazon.com', user_id='A3Q0VJTUO4EZ56')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[score: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Extract lists of `score` and `helpfulness_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Second way to do it\n",
    "scores = df.select(\"score\").rdd.flatMap(lambda x: x).collect()\n",
    "helpfulness =  df.select(\"helpfulness_score\").rdd.flatMap(lambda x: x).collect()\n",
    "\"\"\"\n",
    "\n",
    "df = df.toPandas()\n",
    "scores = df[\"score\"].values.tolist()\n",
    "helpfulness =  df[\"helpfulness_score\"].values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Calculate the Mean\n",
    "\n",
    "Use PySpark to parallelize finding the mean of scores\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\sum_{i = 1}^{n} i}{n}\n",
    "$$\n",
    "\n",
    "Hint: Use accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create RDD\n",
    "rdd = spark_context.parallelize(scores)\n",
    "# Create accumulators()\n",
    "acc = spark_context.accumulator(0)\n",
    "# add them\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "# get back values\n",
    "total = acc.value\n",
    "\n",
    "# calc mean\n",
    "mean_score = total/len(scores)\n",
    "\n",
    "# Print mean\n",
    "print(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.267999999999997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing purpose\n",
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.203401844771729"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing purpose\n",
    "rdd.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Calculate the Mean\n",
    "\n",
    "Use PySpark to parallelize finding the mean of helpfulness score\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\sum_{i = 1}^{n} i}{n}\n",
    "$$\n",
    "\n",
    "Hint: Use accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.973\n"
     ]
    }
   ],
   "source": [
    "# Create RDD\n",
    "rdd = spark_context.parallelize(helpfulness)\n",
    "# Create accumulators()\n",
    "acc = spark_context.accumulator(0)\n",
    "# add them\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "# get back values\n",
    "total = acc.value\n",
    "\n",
    "# calc mean\n",
    "mean_helpfulness_score = total/len(helpfulness)\n",
    "\n",
    "# Print mean\n",
    "print(mean_helpfulness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.972999999999999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing purpose\n",
    "rdd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.031137560513553"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing purpose\n",
    "rdd.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "Calculate the Standard Deviation\n",
    "\n",
    "Use PySpark to parallelize finding the standard deviation of the score. You will most likely need to do 2 separate parallelization calls: one for $(x-\\mu)^2$ and one for the summation\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\n",
    "    \\frac{\\sum_{x = 1}^{n} {(x - \\mu)^2}}{n}\n",
    "}\n",
    "$$\n",
    "\n",
    "Hint: Use accumulator and broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2034018447717287\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "# Create RDD\n",
    "rdd1 = spark_context.parallelize(scores)\n",
    "\n",
    "\n",
    "# create broadcast variables\n",
    "broadcast = spark_context.broadcast(mean_score)\n",
    "\n",
    "# do x - mu ^ 2\n",
    "temp_residual_score = rdd1.map(lambda x: (x - broadcast.value)*(x - broadcast.value))\n",
    "\n",
    "#get the results\n",
    "residual_score = temp_residual_score.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sum the lists\n",
    "rdd2 = spark_context.parallelize(residual_score)\n",
    "acc2 = spark_context.accumulator(0)\n",
    "# add them\n",
    "rdd2.foreach(lambda x: acc2.add(x))\n",
    "# get back values\n",
    "total = acc2.value\n",
    "\n",
    "# divide by n and take sqrt\n",
    "std_score = sqrt(total/len(residual_score))\n",
    "\n",
    "# Print Standard Deviation\n",
    "print(std_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 \n",
    "Now that you have worked through calculating a standard deviation:\n",
    "Calculate the correlation between score and helpfulness score\n",
    "\n",
    "Correlation is given by:\n",
    "$$\n",
    "r_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}\n",
    "$$\n",
    "Where $\\sigma_{xy}$ is the covariance between x and y given by:\n",
    "$$\n",
    "\\sigma_{xy} = \\frac{\n",
    "    \\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y)\n",
    "}{n-1}\n",
    "$$\n",
    "\n",
    "You should parallelize as much as you can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.031137560513553\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# Std Dev of Helpfulness\n",
    "##########\n",
    "# Create RDD\n",
    "rdd1 = spark_context.parallelize(helpfulness)\n",
    "\n",
    "\n",
    "# create broadcast variables\n",
    "broadcast = spark_context.broadcast(mean_helpfulness_score)\n",
    "\n",
    "# do x - mu ^ 2\n",
    "temp_residual_score = rdd1.map(lambda x: (x - broadcast.value)*(x - broadcast.value))\n",
    "\n",
    "#get the results\n",
    "residual_helpfulness = temp_residual_score.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sum the lists\n",
    "rdd2 = spark_context.parallelize(residual_helpfulness)\n",
    "acc2 = spark_context.accumulator(0)\n",
    "# add them\n",
    "rdd2.foreach(lambda x: acc2.add(x))\n",
    "# get back values\n",
    "total = acc2.value\n",
    "\n",
    "# divide by n and take sqrt\n",
    "std_helpfulness = sqrt(total/len(residual_helpfulness))\n",
    "\n",
    "# Print Standard Deviation\n",
    "print(std_helpfulness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34758358358358327\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Covariance between X and Y\n",
    "############\n",
    "# Create RDD\n",
    "rdd = spark_context.parallelize(scores)\n",
    "rdd1 = spark_context.parallelize(helpfulness)\n",
    "\n",
    "\n",
    "# create broadcast variables\n",
    "broadcast_mean_score = spark_context.broadcast(mean_score)\n",
    "broadcast_mean_helpfulness_score = spark_context.broadcast(mean_helpfulness_score)\n",
    "\n",
    "# do x - mu\n",
    "temp_residual_score = rdd.map(lambda x: (x - broadcast_mean_score.value))\n",
    "temp_residual_helpfulness = rdd1.map(lambda x: (x - broadcast_mean_helpfulness_score.value))\n",
    "\n",
    "#get the results\n",
    "residual_score = temp_residual_score.collect()\n",
    "residual_helpfulness = temp_residual_helpfulness.collect()\n",
    "product_score_helpfulnessScore = []\n",
    "for a,b in zip(residual_score,residual_helpfulness):\n",
    "    product_score_helpfulnessScore.append(a*b)\n",
    "\n",
    "\n",
    "# sum the lists\n",
    "rdd2 = spark_context.parallelize(product_score_helpfulnessScore)\n",
    "acc2 = spark_context.accumulator(0)\n",
    "# add them\n",
    "rdd2.foreach(lambda x: acc2.add(x))\n",
    "# get back values\n",
    "total = acc2.value\n",
    "\n",
    "# divide by n -1\n",
    "covariance_score_helpfulnessScore = total/(len(product_score_helpfulnessScore)-1)\n",
    "\n",
    "# Print covariance\n",
    "print(covariance_score_helpfulnessScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02400722104063107\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Calculate correlation\n",
    "##############\n",
    "correlation_score_helpfulnessScore = covariance_score_helpfulnessScore/(std_score*std_helpfulness)\n",
    "print(correlation_score_helpfulnessScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
