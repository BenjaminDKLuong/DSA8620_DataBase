{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:red;font-weight:bold;background:yellow;text-align:center;padding:10px;border:solid\">\n",
    "    <h1>RUN IN EMR CLUSTER ONLY</h1>\n",
    "    If the URL of the current page does not begin with \"ec2\", then do **NOT** proceed!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SparkML Practice\n",
    "In this practice, you will use the tools you learned in the readings and labs to perform some machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = !hostname\n",
    "if \"dsa\" in name[0]:\n",
    "    raise RuntimeError(\"Only run this notebook in the EMR Cluster!\")\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark-lab\")\n",
    "spark_context = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use SparkML to perform a classification task. \n",
    "\n",
    "To do this, we will use the Iris dataset. The Iris dataset is a standard dataset for introductory machine learning. It deals with distinguishing between 3 types of Iris flowers. \n",
    "\n",
    "The dataset contains 4 features. We will import it from a library called `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# To use Spark SQL we create a SQLContext from SparkContext\n",
    "sqlContext = SQLContext(spark_context)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# the data\n",
    "iris_data = iris.data\n",
    "# the labels\n",
    "iris_labels = iris.target\n",
    "\n",
    "# create pandas dataframe\n",
    "pd_df = pd.DataFrame(iris_data)\n",
    "pd_df[\"label\"] = iris_labels\n",
    "\n",
    "# create spark dataframe\n",
    "df = sqlContext.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(0=5.1, 1=3.5, 2=1.4, 3=0.2, label=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "Create the VectorAssembler & Data Partitioning\n",
    "\n",
    "Now, use SparkSQL to create the features and then partition the data into a training set with 80% of the data and a testing set with 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create a vector assembler - this will create a new column that includes columns that are considered \n",
    "# features and assembles them into a vector\n",
    "features = VectorAssembler(\n",
    "    inputCols = [\"0\",\"1\",\"2\",\"3\"],\n",
    "    outputCol = \"features\")\n",
    "\n",
    "# split into Train and Test\n",
    "train_data, test_data = features.transform(df).randomSplit([0.8,0.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "Create the Model\n",
    "\n",
    "Use the `MultilayerPerceptronClassifier`\n",
    "\n",
    "Be sure to pass `layers=[4,5,4,3]` to the `MultilayerPercentronClassifier`'s contructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "layers = [4,5,4,3]\n",
    "trainer = MultilayerPerceptronClassifier(layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = trainer.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "Predict and Evaluate Model\n",
    "\n",
    "Use the trained model to predict the classes of the test data, and then print the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.30%\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data -- the model has not seen in\n",
    "result = model.transform(test_data)\n",
    "\n",
    "#bring in evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#create evaluator\n",
    "predictionAndLabels = result.select(\"label\", \"prediction\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# get the accuracy\n",
    "accuracy = evaluator.evaluate(predictionAndLabels)\n",
    "\n",
    "\n",
    "print(\"{:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "Now that we have seen the MLP, how does the Naive Bayes classifier perform?\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#naive-bayes\n",
    "\n",
    "In the cell below, train and test using the Naive Bayes classifier within `pyspark.ml`. Feel free to experiment with the NB parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 77.77777777777779%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "# predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy*100)+\"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "Use the train data to perform linear regression and then display the coefficients, intercepts, and $r^2$ value after regression \n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.13758625148863807,0.38816100358457084]\n",
      "Intercept: 0.02439025765463954\n",
      "r2: 0.822424\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_data)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
