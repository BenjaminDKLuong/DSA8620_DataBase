{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:red;font-weight:bold;background:yellow;text-align:center;padding:10px;border:solid\">\n",
    "    <h1>RUN IN EMR CLUSTER ONLY</h1>\n",
    "    If the URL of the current page does not begin with \"ec2\", then do **NOT** proceed!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SparkML Lab\n",
    "\n",
    "In this lab, we will do some Machine Learning using PySpark.\n",
    "\n",
    "This is a power library with a lot of feature for numerical data processing.\n",
    "As you may have noticed, traditional data storage systems (DBMS, Data Warehouses, etc.) are not well suited for numerical data processing.\n",
    "\n",
    "The Spark ML Library adds that capability to the Spark environment.\n",
    "\n",
    "Read More [Here](https://spark.apache.org/docs/latest/ml-guide.html).\n",
    "Useful sections include:\n",
    " * https://spark.apache.org/docs/2.3.0/ml-statistics.html\n",
    " * https://spark.apache.org/docs/2.3.0/ml-pipeline.html\n",
    " * https://spark.apache.org/docs/2.3.0/ml-features.html\n",
    " * https://spark.apache.org/docs/2.3.0/ml-classification-regression.html\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = !hostname\n",
    "if \"dsa\" in name[0]:\n",
    "    raise RuntimeError(\"Only run this notebook in the EMR Cluster!\")\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark-lab\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# To use Spark SQL we create a SQLContext from SparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Location of the dataset on HDFS\n",
    "DATASET = '/datasets/titanic.csv'\n",
    "\n",
    "# Load a table with a CSV format reader\n",
    "dataset = sqlContext.read.format('com.databricks.spark.csv').options(\n",
    "    header='true', inferschema='true').load(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Machine Learning Model\n",
    "We will use a decision tree classifier for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create a vector assembler - this will create a new column that \n",
    "#   includes columns that are considered \n",
    "#   features and assembles them into a vector\n",
    "features = VectorAssembler(\n",
    "    inputCols=[\"pclass\", \"sex\", \"age\", \"embarked\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# split into Train and Test\n",
    "train_data, test_data = features.transform(dataset).randomSplit([0.7, 0.3])\n",
    "\n",
    "# create model\n",
    "decision_tree = DecisionTreeClassifier(labelCol=\"survived\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = decision_tree.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.15%\n"
     ]
    }
   ],
   "source": [
    "# predict on the test data -- the model has not seen in\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "#bring in evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#create evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"survived\", predictionCol=\"prediction\")\n",
    "# get the accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"{:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see more of the SparkML library in courses such as Data Mining and Information Retrieval and Applied Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
