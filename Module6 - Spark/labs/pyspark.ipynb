{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:red;font-weight:bold;background:yellow;text-align:center;padding:10px;border:solid\">\n",
    "    <h1>RUN IN EMR CLUSTER ONLY</h1>\n",
    "    If the URL of the current page does not begin with \"ec2\", then do **NOT** proceed!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PySpark\n",
    "\n",
    "This lab will look at using PySpark to parallelize some computation, and then we will look at a concrete example of using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = !hostname\n",
    "if \"dsa\" in name[0]:\n",
    "    raise RuntimeError(\"Only run this notebook in the EMR Cluster!\")\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark-lab\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Resilient Distributed Dataset\n",
    "\n",
    "`sc.parallelize()` will prepare a Resilient Distributed Dataset (RDD) that lives on the cluster. This enables parallel computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the original data\n",
    "data = range(0, 100)\n",
    "# now that we have a list, we can make it parallelized\n",
    "parallel_data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an RDD to Parallelize\n",
    "Now that we have a RDD, we can perform operations such as `map` or `reduce`. \n",
    "\n",
    "For more details as well as the full list of available operations: [click here](https://spark.apache.org/docs/2.1.1/programming-guide.html#rdd-operations)\n",
    "\n",
    "Lets double every odd number and halve every even number in the data. To do this, we must first write the function to take in a single element and return the halved or double value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_or_double(value):\n",
    "    return value / 2 if value % 2 == 0 else value * 2\n",
    "\n",
    "# hand it off\n",
    "transformed_data = parallel_data.map(half_or_double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting and Checking Results\n",
    "\n",
    "Now we use the `collect` function to get the results as a list, then we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> 0\n",
      "1 --> 2\n",
      "2 --> 1\n",
      "3 --> 6\n",
      "4 --> 2\n",
      "5 --> 10\n",
      "6 --> 3\n",
      "7 --> 14\n",
      "8 --> 4\n",
      "9 --> 18\n",
      "10 --> 5\n",
      "11 --> 22\n",
      "12 --> 6\n",
      "13 --> 26\n",
      "14 --> 7\n",
      "15 --> 30\n",
      "16 --> 8\n",
      "17 --> 34\n",
      "18 --> 9\n",
      "19 --> 38\n",
      "20 --> 10\n",
      "21 --> 42\n",
      "22 --> 11\n",
      "23 --> 46\n",
      "24 --> 12\n",
      "25 --> 50\n",
      "26 --> 13\n",
      "27 --> 54\n",
      "28 --> 14\n",
      "29 --> 58\n",
      "30 --> 15\n",
      "31 --> 62\n",
      "32 --> 16\n",
      "33 --> 66\n",
      "34 --> 17\n",
      "35 --> 70\n",
      "36 --> 18\n",
      "37 --> 74\n",
      "38 --> 19\n",
      "39 --> 78\n",
      "40 --> 20\n",
      "41 --> 82\n",
      "42 --> 21\n",
      "43 --> 86\n",
      "44 --> 22\n",
      "45 --> 90\n",
      "46 --> 23\n",
      "47 --> 94\n",
      "48 --> 24\n",
      "49 --> 98\n",
      "50 --> 25\n",
      "51 --> 102\n",
      "52 --> 26\n",
      "53 --> 106\n",
      "54 --> 27\n",
      "55 --> 110\n",
      "56 --> 28\n",
      "57 --> 114\n",
      "58 --> 29\n",
      "59 --> 118\n",
      "60 --> 30\n",
      "61 --> 122\n",
      "62 --> 31\n",
      "63 --> 126\n",
      "64 --> 32\n",
      "65 --> 130\n",
      "66 --> 33\n",
      "67 --> 134\n",
      "68 --> 34\n",
      "69 --> 138\n",
      "70 --> 35\n",
      "71 --> 142\n",
      "72 --> 36\n",
      "73 --> 146\n",
      "74 --> 37\n",
      "75 --> 150\n",
      "76 --> 38\n",
      "77 --> 154\n",
      "78 --> 39\n",
      "79 --> 158\n",
      "80 --> 40\n",
      "81 --> 162\n",
      "82 --> 41\n",
      "83 --> 166\n",
      "84 --> 42\n",
      "85 --> 170\n",
      "86 --> 43\n",
      "87 --> 174\n",
      "88 --> 44\n",
      "89 --> 178\n",
      "90 --> 45\n",
      "91 --> 182\n",
      "92 --> 46\n",
      "93 --> 186\n",
      "94 --> 47\n",
      "95 --> 190\n",
      "96 --> 48\n",
      "97 --> 194\n",
      "98 --> 49\n",
      "99 --> 198\n"
     ]
    }
   ],
   "source": [
    "# get list\n",
    "result_data = transformed_data.collect()\n",
    "# print results\n",
    "for original, transformed in zip(data, result_data):\n",
    "    print(\"{} --> {}\".format(original, int(transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Pi\n",
    "Now that we have seen a very simple example, lets use this new ability to estimate pi, the mathematical constant. \n",
    "\n",
    "Pi can be estimated by throwing darts at a dart board. A more mathematical explanation is [here](http://www.thephysicsmill.com/2014/05/03/throwing-darts-pi/)\n",
    "\n",
    "We will perform the same steps as before to do this, and then we will compare with the actual value of pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data and Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "data = range(0, NUM_SAMPLES)\n",
    "\n",
    "parallel_data = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handoff For Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def throw_dart(value):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x**2 + y**2 < 1\n",
    "\n",
    "# hand off\n",
    "result = parallel_data.filter(throw_dart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Results & Compare to Actual Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.03360000000\n",
      "3.14159265359\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# get count\n",
    "count = result.count()\n",
    "# count is about pi/4, so we need to mult by 4 to get our approx\n",
    "pi_estimation = 4 * count / NUM_SAMPLES\n",
    "\n",
    "print(\"{:.11f}\\n{:.11f}\".format(pi_estimation, math.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our estimation is pretty bad. \n",
    "If we increase the NUM_SAMPLES, then our estimation will get closer. \n",
    "**Feel free to experiment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators and Broadcast\n",
    "\n",
    "Sometimes it is necessary to sum a list. \n",
    "To do this within a Spark Context, we use `accumulators`. While we could just use elementry Python to accomplish the task, we want to leverage the power and parallelization of Spark to perform this task. \n",
    "\n",
    "We can then sum in parallel using the `foreach` method on an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Create List\n",
    "data = range(0,10)\n",
    "# Create RDD\n",
    "rdd = sc.parallelize(data)\n",
    "# Create accumulators\n",
    "acc = sc.accumulator(0)\n",
    "# add them in parallel\n",
    "rdd.foreach(lambda x: acc.add(x))\n",
    "\n",
    "# get the value back\n",
    "total = acc.value # <---the .value gets the accumulators value back\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to accumulators, sometimes we need a broadcasted variable, which is a read-only value that we can read when parallelizing. \n",
    "\n",
    "This is necessary because we want all of our nodes to be able to see the same value in a parallel context. For example, if we want to add the number 5 to every number in our RDD, we could do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "# create broadcast\n",
    "broadcast = sc.broadcast(5)\n",
    "# add 5 to every number\n",
    "temp_result = rdd.map(lambda x: x + broadcast.value)\n",
    "#get the results\n",
    "result = temp_result.collect()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we add the number 5, but that can also be a variable. \n",
    "\n",
    "\n",
    "A use case for this is if you perform some computation and get a number and then want to parallelize some sort of operation using that computed number you could store it in a broadcast variable and then use a Spark RDD to parallelize your next computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background:yellow\">Your Turn - Task 1</span>\n",
    "\n",
    "Create an array of values from 0-49. Then, use the spark cluster to look at each value.\n",
    "If it is **odd**, then return 500. \n",
    "If it is **even**, then return the value minus 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> -1\n",
      "1 --> 500\n",
      "2 --> -1\n",
      "3 --> 500\n",
      "4 --> -1\n",
      "5 --> 500\n",
      "6 --> -1\n",
      "7 --> 500\n",
      "8 --> -1\n",
      "9 --> 500\n",
      "10 --> -1\n",
      "11 --> 500\n",
      "12 --> -1\n",
      "13 --> 500\n",
      "14 --> -1\n",
      "15 --> 500\n",
      "16 --> -1\n",
      "17 --> 500\n",
      "18 --> -1\n",
      "19 --> 500\n",
      "20 --> -1\n",
      "21 --> 500\n",
      "22 --> -1\n",
      "23 --> 500\n",
      "24 --> -1\n",
      "25 --> 500\n",
      "26 --> -1\n",
      "27 --> 500\n",
      "28 --> -1\n",
      "29 --> 500\n",
      "30 --> -1\n",
      "31 --> 500\n",
      "32 --> -1\n",
      "33 --> 500\n",
      "34 --> -1\n",
      "35 --> 500\n",
      "36 --> -1\n",
      "37 --> 500\n",
      "38 --> -1\n",
      "39 --> 500\n",
      "40 --> -1\n",
      "41 --> 500\n",
      "42 --> -1\n",
      "43 --> 500\n",
      "44 --> -1\n",
      "45 --> 500\n",
      "46 --> -1\n",
      "47 --> 500\n",
      "48 --> -1\n",
      "49 --> 500\n"
     ]
    }
   ],
   "source": [
    "## Add your code in the cell below\n",
    "## ---------------------------------\n",
    "\n",
    "data = range(0, 50)\n",
    "# Create RDD\n",
    "parallel_data = sc.parallelize(data)\n",
    "\n",
    "\n",
    "### Handoff for Computation\n",
    "def function500orminus1(value):\n",
    "    return -1 if value % 2 == 0 else 500\n",
    "\n",
    "# hand it off\n",
    "transformed_data = parallel_data.map(function500orminus1)\n",
    "\n",
    "\n",
    "### Gather and View Results\n",
    "# get list\n",
    "result_data = transformed_data.collect()\n",
    "# print results\n",
    "for original, transformed in zip(data, result_data):\n",
    "    print(\"{} --> {}\".format(original, int(transformed)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
