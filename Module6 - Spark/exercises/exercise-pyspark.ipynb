{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:red;font-weight:bold;background:yellow;text-align:center;padding:10px;border:solid\">\n",
    "    <h1>RUN IN EMR CLUSTER ONLY</h1>\n",
    "    If the URL of the current page does not begin with \"ec2\", then do **NOT** proceed!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark Exercise\n",
    "In this exercise, you will use the tools you learned in the readings and labs to perform some computation, some dataset querying, and some machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-excercise, master=yarn) created by __init__ at <ipython-input-2-be3af4cd9708>:9 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-be3af4cd9708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyspark-excercise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 308\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-excercise, master=yarn) created by __init__ at <ipython-input-2-be3af4cd9708>:9 "
     ]
    }
   ],
   "source": [
    "name = !hostname\n",
    "if \"dsa\" in name[0]:\n",
    "    raise RuntimeError(\"Only run this notebook in the EMR Cluster!\")\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"pyspark-excercise\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "For this exercise, you will be using a dataset that describes workers that have missed hours at work for some reason. \n",
    "This dataset is located in HDFS at `/datasets/work-absentees.csv`. Ingest that dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# To use Spark SQL we create a SQLContext from SparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Location of the dataset on HDFS\n",
    "DATASET = '/datasets/work-absentees.csv'\n",
    "\n",
    "# Load a table with a CSV format reader\n",
    "dataset = sqlContext.read.format('com.databricks.spark.csv').options(\n",
    "    header='true', inferschema='true').load(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "To understand what we are looking at, show the columns and their data types in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=11, reason=26, month=7, day=3, season=1, transportation_expense=289, distance=36, service_time=13, age=33, work_load_per_day=239.554, target=97, disciplined=0, education_level=1, number_children=2, social_drinker=1, social_smoker=0, number_pets=1, weight=90, height=172, bmi=30, hours_absent=4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- reason: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- transportation_expense: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- service_time: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- work_load_per_day: double (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- disciplined: integer (nullable = true)\n",
      " |-- education_level: integer (nullable = true)\n",
      " |-- number_children: integer (nullable = true)\n",
      " |-- social_drinker: integer (nullable = true)\n",
      " |-- social_smoker: integer (nullable = true)\n",
      " |-- number_pets: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- bmi: integer (nullable = true)\n",
      " |-- hours_absent: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "#### Using SQL Statements, answer the following:\n",
    "\n",
    "Say the company for which these employees work is trying to understand why these employees are missing work. \n",
    "In the cells below, display the top 5 reasons that workers missed. For this one, you can use LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.createOrReplaceTempView(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|reason|count|\n",
      "+------+-----+\n",
      "|    23|  149|\n",
      "|    28|  112|\n",
      "|    27|   68|\n",
      "|    13|   55|\n",
      "|    19|   40|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT reason,count(*) as count FROM dataset WHERE hours_absent>0 GROUP BY reason ORDER BY count DESC LIMIT 5\"\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What month(s) has the most absent hours? DO NOT USE `LIMIT`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|total|\n",
      "+-----+-----+\n",
      "|    3|  765|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM (SELECT month,SUM(hours_absent) as total FROM dataset GROUP BY month) as table\n",
    "WHERE total = (SELECT MAX(total) FROM (SELECT month,SUM(hours_absent) as total FROM dataset GROUP BY month))\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference in average workload per day between workers with a PhD and workers without one? (education_level = 3 for workers with PhD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        different|\n",
      "+-----------------+\n",
      "|-8.59885378885133|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I get the average from non_PhD workers - the average from PhD worker because I like the positive number\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT (sum(case when education_level =3 then work_load_per_day else 0 end)/sum(case when education_level =3 then 1 else 0 end)) \n",
    "- (sum(case when education_level !=3 then work_load_per_day else 0 end)/sum(case when education_level !=3 then 1 else 0 end)) as different\n",
    "FROM dataset \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many workers have 0 hours absent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      24|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT count(*)\n",
    "FROM (SELECT DISTINCT ID FROM dataset WHERE hours_absent =0) as table\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "Use PySpark to parallelize: What is the correlation between social drinking and social smoking according to this dataset? You must parallelize the calculation of cross covariance, but you may use libraries for mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dataset.toPandas()\n",
    "social_drinker =  df['social_drinker'].values.tolist()\n",
    "social_smoker =  df['social_smoker'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find mean\n",
    "social_drinker_mean = sc.parallelize(social_drinker).mean()\n",
    "social_smoker_mean = sc.parallelize(social_smoker).mean()\n",
    "\n",
    "#find standard deviation\n",
    "social_drinker_stdev = sc.parallelize(social_drinker).stdev()\n",
    "social_smoker_stdev = sc.parallelize(social_smoker).stdev()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is covariance: -0.014409538090187619\n",
      "This is correlation: -0.11182912497430188\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Covariance between social_drinker and social_smoker\n",
    "############\n",
    "# Create RDD\n",
    "rdd = sc.parallelize(social_drinker)\n",
    "rdd1 = sc.parallelize(social_smoker)\n",
    "\n",
    "\n",
    "# create broadcast variables\n",
    "broadcast_social_drinker_mean = sc.broadcast(social_drinker_mean)\n",
    "broadcast_social_smoker_mean = sc.broadcast(social_smoker_mean)\n",
    "\n",
    "# do x - mu\n",
    "temp_residual_drinker = rdd.map(lambda x: (x - broadcast_social_drinker_mean.value))\n",
    "temp_residual_smoker = rdd1.map(lambda x: (x - broadcast_social_smoker_mean.value))\n",
    "\n",
    "#get the results\n",
    "residual_drinker = temp_residual_drinker.collect()\n",
    "residual_smoker = temp_residual_smoker.collect()\n",
    "\n",
    "#create list of production betweeen social_drinker and social_smoker\n",
    "product_drinker_smoker = []\n",
    "for a,b in zip(residual_drinker,residual_smoker):\n",
    "    product_drinker_smoker.append(a*b)\n",
    "\n",
    "\n",
    "# sum the lists\n",
    "rdd2 = sc.parallelize(product_drinker_smoker)\n",
    "acc2 = sc.accumulator(0)\n",
    "# add them\n",
    "rdd2.foreach(lambda x: acc2.add(x))\n",
    "# get back values\n",
    "total = acc2.value\n",
    "\n",
    "# divide by n -1\n",
    "covariance_drinker_smoker = total/(len(product_drinker_smoker)-1)\n",
    "print(\"This is covariance: \" + str(covariance_drinker_smoker))\n",
    "\n",
    "\n",
    "##############\n",
    "# Calculate correlation\n",
    "##############\n",
    "correlation_drinker_smoker = covariance_drinker_smoker/(social_drinker_stdev*social_smoker_stdev)\n",
    "print(\"This is correlation: \" + str(correlation_drinker_smoker))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 \n",
    "Use PySpark to parallelize the following:\n",
    "    Find the median of the number of hours absent. \n",
    "    \n",
    "To do this, you will be using the Median of Medians algorithm.\n",
    "1. Split the data into 5 equal partitions\n",
    "2. Hand off each partition and find the median of that partition \n",
    "3. Collect the 5 medians\n",
    "4. Find the medians of the 5 medians\n",
    "\n",
    "The result of step 4 will be approximately the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_absent =  df['hours_absent'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the data into 5 equal partions\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(hours_absent, 5)\n",
    "partitions = rdd.glom().collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nANOTHER WAY TO PARTITION\\n\\n\\nsplit_points = [int(i*partition_size) for i in range(0, number_of_partition+1)]\\n\\npartitions = []\\nfor i in range(len(split_points)-1):\\n    partition = hours_absent[split_points[i]:split_points[i+1]]\\n    partitions.append(partition)\\n    \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ANOTHER WAY TO PARTITION\n",
    "\n",
    "number_of_partition = 5\n",
    "partition_size = len(hours_absent)/number_of_partition\n",
    "\n",
    "\n",
    "split_points = [int(i*partition_size) for i in range(0, number_of_partition+1)]\n",
    "\n",
    "partitions = []\n",
    "for i in range(len(split_points)-1):\n",
    "    partition = hours_absent[split_points[i]:split_points[i+1]]\n",
    "    partitions.append(partition)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of medians:  [[3.0, 8.0, 3.0, 3.0, 3.0]]\n",
      "The final median:  3.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Hand off each partition and find the median of that partition\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "medians = []\n",
    "rdd = sc.parallelize(partitions)\n",
    "temp_result = rdd.map(lambda x: np.median(x))\n",
    "\n",
    "# 3. Collect 5 medians\n",
    "medians.append(temp_result.collect())\n",
    "\n",
    "print(\"List of medians: \",medians)\n",
    "\n",
    "# 4.Find the medians of the 5 medians\n",
    "median = np.median(medians)\n",
    "print(\"The final median: \",median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\n",
    "So now we know the Top 5 reasons workers missed and the mode of the number of hours missed.\n",
    "Lets now try to do some Machine Learning to see if we can use certain elements to predict the number of hours someone will miss.\n",
    "\n",
    "In the cells below, try 2 different methods to predict the number of hours a worker will miss. The two methods can have differing input columns for the prediction or a differing type of machine learning model or both. \n",
    "\n",
    "The only requirements are:\n",
    "1. Split the data into an 80/20 random split for training/testing, respectively\n",
    "2. Generate a percent accuracy on the testing set for two differing ways of predicting using the Spark ML library.\n",
    "\n",
    "You will need to create a column named \"label\" inside the Spark Dataset for previous code to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## break into categories\n",
    "## 0      #0  0 hours absent\n",
    "## 1-8    #1  <1 day\n",
    "## 9-24   #2  1-3 days\n",
    "## 25-56  #3  3-7 days\n",
    "## 57-112 #4  7-14 days\n",
    "## >113   #5  >14 days\n",
    "\n",
    "bins = [1,8,24,56,112,125]\n",
    "labels=[1,2,3,4,5]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df['label'] = pd.cut(df['hours_absent'], bins=bins, labels=labels, include_lowest=True)\n",
    "df['label'] = pd.to_numeric(df['label']).fillna(0)\n",
    "\n",
    "#convert back to spark dataframe\n",
    "spark_df = SQLContext(sc).createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create a vector assembler - this will create a new column that includes columns that are considered \n",
    "# features and assembles them into a vector\n",
    "features = VectorAssembler(\n",
    "    inputCols = [\"reason\",\"season\",\"transportation_expense\",\"distance\",\"service_time\",\"age\",\"work_load_per_day\",\"education_level\",\"number_children\",\"social_drinker\",\"social_smoker\"],\n",
    "    outputCol = \"features\")\n",
    "\n",
    "# split into Train and Test\n",
    "train_data, test_data = features.transform(spark_df).randomSplit([0.8,0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 84.21%\n"
     ]
    }
   ],
   "source": [
    "# MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "#bring in evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "layers = [11,5,4,6]\n",
    "trainer = MultilayerPerceptronClassifier(layers=layers)\n",
    "model = trainer.fit(train_data)\n",
    "\n",
    "# predict on the test data -- the model has not seen in\n",
    "result = model.transform(test_data)\n",
    "\n",
    "\n",
    "#create evaluator\n",
    "predictionAndLabels = result.select(\"label\", \"prediction\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# get the accuracy\n",
    "accuracy = evaluator.evaluate(predictionAndLabels)\n",
    "\n",
    "\n",
    "print(\"Test set accuracy = {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 63.16%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "# predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test set accuracy = {:.2f}%\".format(accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
